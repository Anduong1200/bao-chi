# Flash News Hunter ğŸ“°âš¡

**"Capture First, Review Later"** - Báº¯t tin nhanh trÆ°á»›c khi bá»‹ xÃ³a.

## Váº¥n Ä‘á»
BÃ¡o chÃ­ hiá»‡n Ä‘áº¡i thÆ°á»ng Ä‘Äƒng bÃ i "thÄƒm dÃ²" rá»“i xÃ³a sau 5-10 phÃºt. Tool thÃ´ng thÆ°á»ng khÃ´ng báº¯t Ä‘Æ°á»£c vÃ¬ khi click vÃ o thÃ¬ link Ä‘Ã£ cháº¿t (404).

## Giáº£i phÃ¡p
Há»‡ thá»‘ng tá»± Ä‘á»™ng táº£i HTML **ngay láº­p tá»©c** khi phÃ¡t hiá»‡n URL má»›i, lÆ°u offline Ä‘á»ƒ Ä‘á»c sau.

---

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run GUI
python gui.py

# Or run capture loop only (headless)
python main.py
```

---

## ğŸ“ Project Structure

```
crawl/
â”œâ”€â”€ gui.py           # Triage UI (Stream/Reading Box/Archive)
â”œâ”€â”€ main.py          # FlashNewsHunter orchestrator
â”œâ”€â”€ archiver.py      # Auto-capture on URL detection
â”œâ”€â”€ scanner.py       # RSS/Sitemap scanner
â”œâ”€â”€ parser.py        # HTML parser
â”œâ”€â”€ storage.py       # SQLite with triage status
â”œâ”€â”€ config.yaml      # Sources configuration
â”œâ”€â”€ config.py        # Config loader
â”œâ”€â”€ alerter.py       # Telegram alerts
â”œâ”€â”€ worker.py        # Worker pool
â””â”€â”€ data/
    â””â”€â”€ articles.db  # SQLite database
```

---

## ğŸ¯ Core Features

### 1. Capture First
- Scan sources every 3-5 seconds
- IMMEDIATELY fetch + save HTML when new URL detected
- No waiting for user interaction

### 2. Triage Workflow (3 Tabs)

| Tab | Status | Actions |
|-----|--------|---------|
| âš¡ **Stream** | `new` (0) | [Pick] â†’ Reading Box |
| ğŸ“– **Reading Box** | `picked` (1) | [Save] / [Discard] |
| ğŸ“ **Archive** | `archived` (2) | Export, Search |

### 3. Link Status Tracking
- ğŸŸ¢ Live - Link cÃ²n sá»‘ng
- ğŸ”´ Dead - Link Ä‘Ã£ cháº¿t (váº«n Ä‘á»c Ä‘Æ°á»£c tá»« cache)

### 4. Image Tracking
- Má»—i áº£nh cÃ³ ID riÃªng gáº¯n vá»›i `article_id`
- CÃ³ thá»ƒ táº£i áº£nh vá» local sau

### 5. DB Control
- **Export Full DB** - Backup toÃ n bá»™
- **Import DB** - Merge hoáº·c Replace

---

## âš™ï¸ Configuration

Edit `config.yaml` to add/modify sources:

```yaml
sources:
  - name: "ThanhNien_TrangChu"
    url: "https://thanhnien.vn/rss/home.rss"
    type: rss
    site_code: TNO
    enabled: true
    frequency: 60
```

---

## ğŸ“Š Database Schema

**Articles Table:**
```sql
articles (
    id TEXT PRIMARY KEY,
    source_name TEXT,
    url TEXT UNIQUE,
    title TEXT,
    content_html TEXT,
    status INTEGER,      -- 0=new, 1=picked, 2=archived, -1=discarded
    link_alive INTEGER,  -- 1=alive, 0=dead
    crawled_at TEXT
)
```

**Images Table:**
```sql
images (
    id TEXT PRIMARY KEY,
    article_id TEXT,     -- Foreign key
    url TEXT,
    local_path TEXT,
    downloaded INTEGER
)
```

---

## ğŸ”§ API Reference

```python
from storage import get_storage

storage = get_storage()

# Triage
stream = storage.get_stream()           # Get new articles
storage.pick_article(article_id)        # Move to Reading Box
storage.archive_article(article_id)     # Save permanently
storage.discard_article(article_id)     # Throw away

# Images
storage.save_image(article_id, img_url)
images = storage.get_article_images(article_id)

# Backup
storage.export_full_db("backup.json")
storage.import_db("backup.json", merge=True)
```

---

## ğŸ“‹ Workflow

```
1. Scanner detects new URL
2. Archiver IMMEDIATELY fetches HTML
3. Saved to DB with status=new
4. Appears in Stream tab
5. User clicks [Pick] â†’ Reading Box
6. User reads from CACHE (works even if link is dead!)
7. User clicks [Save] â†’ Archive
```

---

## ğŸ“ License

MIT License
